# 线性回归方程

$$
y^{\prime} = b + \omega_1x_1
$$

- $y^{\prime}$是预测的标签，即 output
- $b$是模型的偏差，是模型参数，在训练期间计算得出
- $\omega_1$是特征的权重，是模型参数，在训练期间计算得出
- $x_1$是特征，即 input

  ![1741969935798](image/线性回归/1741969935798.png)

  具有多个特征的模型：

$$
y^{\prime} = b + \omega_1x_1 + \omega_2x_2 + \omega_3x_3 +...+ \omega_nx_n
$$

$$


$$

# 损失函数

- Loss 衡量预测值与实际值之间的差异，侧重于值之间的 *距离* ，而不是方向。

## 损失类型

![1741969935798](image/线性回归/1741969935798.png)

## 如何选择Loss函数

> 取决于dataset和希望处理特定预测的方式。

- 离群值：数据集中的大多数特征值通常属于一个特定范围，超出了典型范围的就叫离群值。
  - 离群值可以指模型的预测与真实值之间的差距
  - MSE 会使模型更接近离群值，而 MAE 则不会。与 L~1~ 损失函数相比，L~2~ 损失函数对离群值的惩罚更高。
- 注意模型与数据之间的关系：
  - **MSE** 模型更接近离群值，但与大多数其他数据点的距离更远。
  - **MAE** 模型离离群值较远，但离大多数其他数据点较近。

# Gradient Descent

> 通过迭代寻找使loss最低的模型的权重和偏差

![1741970570949](image/线性回归/1741970570949.png)

![1741970584051](image/线性回归/1741970584051.png)

# 超参数

> 用于控制训练不同方面的变量,是由程序员来设置和控制的。

## 学习率

    决定了在梯度下降过程的每一步中对权重和偏差进行的更改幅度。模型会将梯度乘以学习率，以确定下一次迭代的模型参数（权重和偏差值）。在[梯度下降](https://developers.google.cn/machine-learning/crash-course/linear-regression/gradient-descent?hl=zh-cn)的第三步中，向负斜率方向移动的“小量”是指学习速率。

- 旧模型参数与新模型参数之间的差异与损失函数的斜率成正比。例如，如果斜率较大，模型会迈出较大的步伐。如果小，则只需迈出小步。
- 学习率过低可能会需要过多的迭代才能收敛
- 学习速率过高时，模型永远不会收敛，因为每次迭代都会导致损失值波动或持续增加。

## 批次大小

> 表示模型在更新权重和偏差之前处理的[**示例**](https://developers.google.cn/machine-learning/glossary?hl=zh-cn#example)数量

我们理所应当地认为：模型应先计算数据集中*每个*示例的损失，然后再更新权重和偏差。但如果数据集包含数十万甚至数百万个示例，使用完整批处理是不切实际的。

以下两种常用技术可在*平均*情况下获得正确的梯度，而无需在更新权重和偏差之前查看数据集中的每个示例，这两种技术分别是[**随机梯度下降**](https://developers.google.cn/machine-learning/glossary?hl=zh-cn#mini-batch-stochastic-gradient-descent)和[**小批量随机梯度下降**](https://developers.google.cn/machine-learning/glossary?hl=zh-cn#mini-batch-stochastic-gradient-descent)：

### 随机梯度下降法SGD

“随机”一词表示每个批次包含的一个示例是随机选择的。每次迭代只使用一个示例（批量大小为 1）。

在足够的迭代次数下，SGD 会起作用，但噪声很大。“噪声”是指训练期间的变化，会导致在迭代过程中损失增加而不是减少。

但适当的噪声可以帮助模型更好地泛化，在神经网络中找到最佳权重和偏差。

### 小批次随机梯度下降法mini-batch SGD

全批次梯度下降法和 SGD 之间的折衷方案。

对于 N 个数据点，批处理大小可以是任何大于 1 且小于 N 的数字。模型会随机选择每个批处理中包含的示例，对其梯度求平均值，然后每迭代一次更新权重和偏差。

每个批次的示例数量如何确定取决于数据集和可用的计算资源。通常，批量大小较小时，其行为类似于 SGD；批量大小较大时，其行为类似于全批梯度下降。

## 周期数

> 在训练期间，一个[**周期**](https://developers.google.cn/machine-learning/glossary?hl=zh-cn#epoch)表示模型已处理训练集中的每个示例*一次* 。
